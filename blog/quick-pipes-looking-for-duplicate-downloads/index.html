<!DOCTYPE html>
<html lang="en-us">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="author" content="Sophie Haskins">
<meta name="description" content="I&rsquo;ve been looking to &ldquo;figure out&rdquo; my system for storing photos lately. I have an iPhone and send its photos to Google Photos, and a Sony mirrorless camera and store its photos in a local Adobe Lightroom directory. Both of these come with monthly subscriptions that I don&rsquo;t really want to pay, especially because I don&rsquo;t really care about the cloud features of either (I have a home NAS). The direction I&rsquo;m experimenting with is Photos on a Mac with a handful of separate photo libraries (&ldquo;personal&rdquo;, pizzabox photos, very old/archive photos, homework for my online photography class) to make it easier to back up / move them around if they get too big.">

<meta property="og:title" content="Quick Pipes: Looking for duplicate downloads" />
<meta property="og:description" content="I&rsquo;ve been looking to &ldquo;figure out&rdquo; my system for storing photos lately. I have an iPhone and send its photos to Google Photos, and a Sony mirrorless camera and store its photos in a local Adobe Lightroom directory. Both of these come with monthly subscriptions that I don&rsquo;t really want to pay, especially because I don&rsquo;t really care about the cloud features of either (I have a home NAS). The direction I&rsquo;m experimenting with is Photos on a Mac with a handful of separate photo libraries (&ldquo;personal&rdquo;, pizzabox photos, very old/archive photos, homework for my online photography class) to make it easier to back up / move them around if they get too big." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.sophaskins.net/blog/quick-pipes-looking-for-duplicate-downloads/" />
<meta property="article:published_time" content="2019-11-05T11:00:00-05:00" />
<meta property="article:modified_time" content="2019-11-05T11:00:00-05:00" />


<title>


     Quick Pipes: Looking for duplicate downloads 

</title>
<link rel="canonical" href="https://blog.sophaskins.net/blog/quick-pipes-looking-for-duplicate-downloads/">







<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">




<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500">



    
    <link rel="stylesheet" href="https://blog.sophaskins.net/css/reset.css?t=2020-03-17%2011%3a47%3a54.974492%20-0400%20EDT%20m%3d%2b0.123357683">
    <link rel="stylesheet" href="https://blog.sophaskins.net/css/pygments.css?t=2020-03-17%2011%3a47%3a54.974492%20-0400%20EDT%20m%3d%2b0.123357683">
    <link rel="stylesheet" href="https://blog.sophaskins.net/css/main.css?t=2020-03-17%2011%3a47%3a54.974492%20-0400%20EDT%20m%3d%2b0.123357683">
    




<link rel="shortcut icon"

    href="https://blog.sophaskins.net/img/leaf.ico"

>








</head>


<body lang="">

<section class="header">
    <div class="container">
        <div class="content">
            
            <a href="https://blog.sophaskins.net/"><div class="name">Sophie Haskins</div></a>
            
              <h3 class="self-intro">blog.sophaskins.net | personal essays</h3>
            
            <nav>
                <ul>
                    
                        <li class="nav-blog"><a href="https://blog.sophaskins.net/blog/"><span>Blog</span></a></li>
                    
                        <li class="nav-about"><a href="https://blog.sophaskins.net/about/"><span>About</span></a></li>
                    
                        <li class="nav-code"><a href="https://blog.sophaskins.net/code/"><span>Code</span></a></li>
                    
                </ul>
            </nav>
        </div>
    </div>
</section>

<section class="icons">
    <div class="container">
        <div class="content">

        
            <a href="//github.com/sophaskins" target="_blank" rel="noopener"><img class="icon" src="https://blog.sophaskins.net/img/github.svg" alt="github" /></a>
        

        
            <a href="//twitter.com/sophaskins" target="_blank" rel="noopener"><img class="icon" src="https://blog.sophaskins.net/img/twitter.svg" alt="twitter" /></a>
        

        

        

        

        

        

        

        

        

        
            <a href="https://blog.sophaskins.net/index.xml"><img class="icon" src="https://blog.sophaskins.net/img/rss.svg" alt="rss" /></a>
        
        

        <div class="rc-scout"></div>

        </div>
    </div>
</section>


<section class="main post non-narrow zero-top-spacing">
    <div class="container">
        <div class="content">
            <div class="front-matter">
                <div class="title-container">
                    <div class="page-heading">

    Quick Pipes: Looking for duplicate downloads

</div>

                    <div class="initials"><a href="https://blog.sophaskins.net/"></a></div>
                </div>
                <div class="meta">
                    
                    <div class="date" title='Tue Nov 5 2019 11:00:00 EST'>Nov 5, 2019</div>
                    
                    
		    <div class="reading-time"><div class="middot"></div>8 minutes read</div>
                    
                </div>
            </div>
            <div class="markdown">
                <p>I&rsquo;ve been looking to &ldquo;figure out&rdquo; my system for storing photos lately. I have an iPhone and send its photos to <a href="https://www.google.com/photos/about/">Google Photos</a>, and a Sony mirrorless camera and store its photos in a local <a href="https://www.adobe.com/products/photoshop-lightroom.html">Adobe Lightroom</a> directory. Both of these come with <strong>monthly subscriptions that I don&rsquo;t really want to pay</strong>, especially because I don&rsquo;t really <em>care</em> about the cloud features of either (I have a home NAS). The direction I&rsquo;m experimenting with is <a href="https://www.apple.com/macos/photos/">Photos on a Mac</a> with a handful of <a href="https://support.apple.com/en-us/HT201517">separate photo libraries</a> (&ldquo;personal&rdquo;, pizzabox photos, very old/archive photos, homework for my online photography class) to make it easier to back up / move them around if they get too big.</p>
<p>That means <strong>I want to download all my photos from Google and import them</strong>. I ran in to a few obstacles doing that, and wanted to share the Unix command-line tricks I used to get around them!</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>Since I&rsquo;m posting &ldquo;something fun I did on the command-line&rdquo; on The Internet (a place notoriously unforgiving to any imperfections), I want to lay out some things before we start:</p>
<ul>
<li>This is <strong>not the most efficient</strong> / best / easiest way to solve this problem, in terms of keystrokes, CPU time, memory usage, SSD wear, wall-clock time, open-source license of tools involved, viability for people using Be OS on the desktop in 2019, or theoretical cleverness</li>
<li>I <strong>tried a bunch of things that didn&rsquo;t work</strong> while I was building this up. I&rsquo;m only showing what <em>worked</em>, but rest assured, I don&rsquo;t get things right on the first try</li>
<li>I also <strong>don&rsquo;t have any of this memorized</strong> - I did a lot of Googling and manpage reading</li>
<li>I know that I <em>can</em> turn off the auto-unzip-and-delete in Safari, I just hadn&rsquo;t done it yet b/c I recently wiped this computer</li>
</ul>
<p>I&rsquo;m sharing this because I like talking about the thought process behind solutions. It&rsquo;s not a &ldquo;how-to&rdquo;, it&rsquo;s a <em>story</em>.</p>
<h2 id="downloading">Downloading</h2>
<p>Google lets you export your data with their <a href="https://takeout.google.com/settings/takeout/">Takeout</a> service. My photos were a total of ~35GB, which means that they split it up in to multiple 2GB chunks. This was pretty annoying - even with 100 Mbit/s Internet that&rsquo;s a <a href="https://www.wolframalpha.com/input/?i=35+gigabytes+%2F+100+megabits%2Fs">minimum of 45 minutes</a> to download it all! Plus, I needed to keep clicking the next file once each finished:</p>
<p><img src="https://blog.sophaskins.net/img/google-takeout.png" alt="google takeout"></p>
<p>Each part of the download was named something like <code>takeout-20191105T000000Z-001.zip</code>, and there were 21 of them. Before I start importing files to the Photos app, <strong>I want to make sure I downloaded all 21 zip files</strong>. It&rsquo;s easy to verify that I downloaded <em>some</em> 21 files - after all, there are 21 items in my <code>Downloads</code> directory now! But unforunately, Safari immediately unzipped the files after downloading and deleted the zip files. The resulting directories were all named like <code>Takeout-2</code>:</p>
<p><img src="https://blog.sophaskins.net/img/downloads-dir.png" alt="downloads folder"></p>
<p>So as long as I have no duplicate directories, we&rsquo;re fine, right? <strong>Unforunately, no</strong> - the numbers here are added by the Finder as it unzips, because each zip file had a folder called &ldquo;Takeout&rdquo; in it. So, we&rsquo;ve reached the problem - <strong>how can I determine if I accidentally downloaded any zip files twice</strong> (and thus missed some other zip)?</p>
<h2 id="what-do-i-have">What do I have?</h2>
<p>My first step was to figure out what I <em>have</em>. It&rsquo;s a bunch of pictures, living in files that look like: <code>./Takeout/Google Photos/2019-05-07/IMG_1277.JPG</code>. I don&rsquo;t want to look at them all and compare visually, since there&rsquo;s so many!. Even asking a computer to compare each file against eachother would take a <em>very</em> long time. Instead, I used <code>md5</code> (<a href="https://www.freebsd.org/cgi/man.cgi?query=md5&amp;manpath=FreeBSD+12.0-RELEASE+and+Ports">manpage</a>) on each file to calculate a hash. The command I ran was:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">find . -type f -exec md5 <span class="o">{}</span> <span class="se">\;</span> &gt; md5s
</code></pre></div><p>There wasn&rsquo;t anything else in my <code>Downloads</code> directory at the time, so the <code>find . type f</code> lists <em>all</em> the file paths of the photos. Using <code>-exec md5 {} \;</code> tells the <code>find</code> command to run <code>md5 FOO</code> for each <code>FOO</code> that it found. If I were a super command-line-witch, I <em>might</em> have piped this in to some of the next commands, but for me its nice to store the intermediate results in to a file <code>md5s</code> so I can try out a bunch of approaches. The resulting lines in the <code>md5s</code> file look something like:</p>
<pre><code>MD5 (./Takeout-2/Google Photos/2017-12-18/IMG_0555.MOV.json) = 81156046955443c089ae4fc74100998f
</code></pre><h2 id="what-files-are-duplicates">What files are duplicates?</h2>
<p>I definitely expect <em>some</em> duplicate photos - maybe ones I uploaded to Google twice, or exist in multiple album folders, etc. If this weren&rsquo;t the case, we could be done after doing something like</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">awk -F<span class="o">=</span> <span class="s1">&#39;{print $2}&#39;</span> md5s <span class="p">|</span> sort <span class="p">|</span> uniq -c 
</code></pre></div><p>In that command, <code>-F=</code> says &ldquo;split up on the <code>=</code> character&rdquo;, so <code>awk</code> sees the fields of each line from <code>md5s</code> as <code>MD5 (./Takeout-2/Google Photos/2017-12-18/IMG_0555.MOV.json)</code> and <code>81156046955443c089ae4fc74100998f</code>. The <code>'{print $2}'</code> says &ldquo;just show me the second field&rdquo; (ie, the <code>81156046955443c089ae4fc74100998f</code>), and <code>sort | uniq -c</code> shows me the <em>count</em> of each unique value. If any hashes show up twice, we <a href="https://en.wikipedia.org/wiki/Collision_(computer_science)">probably</a> have duplicates! We can find the ones with two entries by adding a little extra to the pipeline:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">awk -F<span class="o">=</span> <span class="s1">&#39;{print $2}&#39;</span> md5s<span class="p">|</span> sort <span class="p">|</span> uniq -c <span class="p">|</span> awk <span class="s1">&#39;$1 &gt; 1 {print $2}&#39;</span>
</code></pre></div><p>Here we do a second <code>awk</code> on the output of <code>uniq -c</code> that, when the count is more than 1 (<code>$1 &gt; 1 </code>), prints out the hash (<code>{print $2}</code>).</p>
<p>But like I said, I <em>expect</em> duplicates. We&rsquo;re not trying to dedupe the files per-se, but make sure that I didn&rsquo;t duplicate an entire <em>folder</em>. If we look at all the duplicate files, we should be able to tell if they&rsquo;re from duplicate folders if everything after the <code>Takeout-2</code> part of the filename is duplicated (for example, if I had downloaded the same zip twice, and it unpacked in to <code>Takeout-2</code> and <code>Takeout-3</code>). The <code>awk</code> above <strong>gets us the hash</strong> of the potentially duplicated files, but <strong>not their full paths</strong>. To get their paths again, we can search back into the <code>md5s</code> file:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">awk -F<span class="o">=</span> <span class="s1">&#39;{print $2}&#39;</span> md5s<span class="p">|</span> sort <span class="p">|</span> uniq -c <span class="p">|</span> awk <span class="s1">&#39;$1 &gt; 1 {print $2}&#39;</span> <span class="p">|</span> xargs -n <span class="m">1</span> -I <span class="s1">&#39;{}&#39;</span> grep <span class="o">{}</span> md5s &gt; dupes
</code></pre></div><p>Here is the fun stuff! Piping to <code>xargs</code> lets us send the hashes right back to a <code>grep</code> to get their full entry from <code>md5s</code>, path and all. The important parts here are that we tell <code>xargs</code> to only do one input at a time with the <code>-n 1</code> (as opposed to doing a big batch of them, which wouldn&rsquo;t be a valid regexp), and then tell it where we want that one value to go in the resulting command with the <code>-I '{}' grep {} md5s</code>. For each hash piped to <code>xargs</code>, it will run</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">grep abcdef0123456789 md5s
</code></pre></div><p>This lets us build up <code>dupes</code>, a version of <code>md5s</code> that only has potentially duplicated files in it.</p>
<h2 id="final-stretch">Final stretch</h2>
<p>Awesome! So we know that everything in <code>dupes</code> has a matching <code>md5</code> hash to some other file - how do we tell if it looks like it&rsquo;s from the same zip file? If it <em>were</em>, the path (minus the <code>Takeout-2</code> part) will be the same, so lets grab just <em>that</em> and see if we have any duplicates:</p>
<pre><code>awk -F/ '{print $4 &quot;/&quot; $5}' dupes | sort | uniq -d
</code></pre><p>Since all the paths start at my downloads folder, if we use <code>-F/</code> to tell <code>awk</code> to split on the <code>/</code> in the path names, I can print out just the latter bit. For:</p>
<pre><code>MD5 (./Takeout-2/Google Photos/2017-12-18/IMG_0555.MOV.json)
</code></pre><p>we end up printing out:</p>
<pre><code>2017-12-18/IMG_0555.MOV.json)
</code></pre><p>Sending <em>that</em> in to <code>sort | uniq -d</code> uses the <code>uniq</code> program to tell us &ldquo;are any two lines in this file the same&rdquo;.</p>
<p>My luck was good! There were no duplicate zip files unpacked here, so I have all the photos!</p>
<h2 id="combine-the-directories">Combine the directories</h2>
<p>Since we now know there&rsquo;s no duplicate paths, I want to combine all the parts in to one big folder. There&rsquo;s a few ways to do this, but it turns out that &ldquo;combining a bunch of directories in to one&rdquo; is something that <code>rsync</code> is good at! I did:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">mkdir combined
<span class="k">for</span> dir in <span class="sb">`</span>ls <span class="p">|</span> grep Takeout<span class="sb">`</span><span class="p">;</span> <span class="k">do</span> rsync -avh <span class="nv">$dir</span> combined<span class="p">;</span> <span class="k">done</span>
</code></pre></div><p>The <code>ls | grep Takeout</code> gives me the full list of unpacked directories (if I just do <code>ls Takeout*</code> the shell unhelpfully expands that wildcard and shows me the <em>contents</em> of each; if I only did <code>ls</code> then I&rsquo;d have picked up <code>md5s</code> and <code>dupes</code>). We loop over those, and <code>rsync -avh $dir combined</code> to copy it from the source dir to <code>combined</code>. Now, <code>combined</code> has one big directory of the full dump from Google Photos!</p>
<h2 id="sophie-this-is-overkill">Sophie this is overkill</h2>
<p>OK so now that I&rsquo;m done, I see some ways I could have made this faster. I didn&rsquo;t need to grab the md5s at all - the paths should have been sufficient! At the beginning I wasn&rsquo;t sure what the structure of the directories was, so I began with <code>md5</code>. Armed with knowledge about the structure, though, I bet we could have done something like:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">find . -type f <span class="p">|</span> awk -F/ <span class="s1">&#39;{print $4&#34;/&#34; $5}&#39;</span> <span class="p">|</span> sort <span class="p">|</span> uniq -d
</code></pre></div><p>and realized things were a-ok. Maybe you have ideas for other cool ways to check it out!</p>

                <br>
		<p><a href="https://blog.sophaskins.net/blog/">Back to posts</a></p>
            </div>
            <br>
            <div class="disqus">
                
            </div>
            
        </div>
    </div>
</section>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-91739270-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  

  <script type="text/javascript">
    hljs.initHighlightingOnLoad();
  </script>




<script async defer src="https://www.recurse-scout.com/loader.js?t=1738b8517df540c1b02fd350f5517117"></script>


</body>
</html>

